---
title: "TP Final"
output: html_document
date: "2025-07-18"
---

Antes de comenzar cargaremos todas las librerías necesarias para tenerlas todas en un solo lugar.
```{r}
#Cargo librerias
library(tidyverse)
library(dplyr)
library(readxl)
library(ggplot2)
library(terra)
library(sf)
library("leaflet")
```

Ahora cargamos los datos correspondientes a los distintos años y los juntamos todos en un mismo dataset.
```{r}
#Cargo datos
datos2022 <- read.csv("./delitos_2022.csv")
datos2023 <- read_csv("./delitos_2023.csv")
datos2024 <- read.csv("./delitos_2024.csv")

#Junto todo
datos2022$fecha <- as.Date(datos2022$fecha)
datos2023$fecha <- as.Date(datos2023$fecha)
datos2024$fecha <- as.Date(datos2024$fecha)
datos2022$franja <- as.numeric(datos2022$franja)
datos2023$franja <- as.numeric(datos2023$franja)
datos2024$franja <- as.numeric(datos2024$franja)

datos2022 <- datos2022 %>% dplyr::select(-id.mapa)
datos2023 <- datos2023 %>% dplyr::select(-"id-mapa")
datos2024 <- datos2024 %>% dplyr::select(-id.mapa)
```
Juntamos todos

```{r}
delitos <- bind_rows(datos2022,datos2023,datos2024)
```

Ignoramos los que tengan NULL en la columna barrio y comuna y nos quedamos solo con un subconjunto de los delitos: Robo total, Hurto total, Robo automotor y Hurto automotor. Que sea total implica que incluye robos o hurtos con arma y sin arma.

```{r}
delitos <- delitos %>% filter(barrio != "NULL")
delitos <- delitos %>% filter(comuna != "NULL")
delitos <- delitos %>% filter(subtipo == "Robo total" | subtipo == "Hurto total" | subtipo == "Robo automotor" | subtipo == "Hurto automotor")
# Vemos que no tengamos NAs
any(is.na(delitos))
```
Contamos la cantidad de los distintos tipos de delitos por barrio.

```{r}
tabla_resumen <- delitos %>%
  filter(barrio != "NULL" & !is.na(barrio)) %>%
  count(barrio, subtipo) %>%
  pivot_wider(names_from = subtipo, values_from = n, values_fill = 0) %>%
  mutate(total_delitos = rowSums(across(where(is.numeric))))

tabla_resumen <- tabla_resumen %>%
  arrange(desc(total_delitos))
tabla_resumen
```

Veamos si efectivamente Palermo es el barrio donde se produce la mayor cantidad de delitos.

```{r}
delitos_largos <- tabla_resumen %>%
  pivot_longer(cols = -barrio, names_to = "tipo_delito", values_to = "cantidad")
```

```{r}
delitos_subset_hurto <- tabla_resumen %>%
  select(barrio, "Hurto total", "Hurto automotor") %>%
  pivot_longer(cols = -barrio, names_to = "tipo_hurto", values_to = "cantidad")

ggplot(delitos_subset_hurto, aes(x = barrio, y = cantidad, fill = tipo_hurto)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Hurto y Hurto Automotor por barrio",
       x = "Barrio", y = "Cantidad") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust=1, vjust=0.4, size=6))
```

```{r}
delitos_subset_robo <- tabla_resumen %>%
  select(barrio, "Robo total", "Robo automotor") %>%
  pivot_longer(cols = -barrio, names_to = "tipo_robo", values_to = "cantidad")

ggplot(delitos_subset_robo, aes(x = barrio, y = cantidad, fill = tipo_robo)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Robo y Robo Automotor por barrio",
       x = "Barrio", y = "Cantidad") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust=1, vjust=0.4, size=6))
```

```{r}
delitos_subset_total <- tabla_resumen %>%
  select(barrio, "total_delitos") %>%
  pivot_longer(cols = -barrio, names_to = "total_delitos", values_to = "cantidad")

ggplot(delitos_subset_total, aes(x = barrio, y = cantidad, fill = total_delitos)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Total delitos por barrio",
       x = "Barrio", y = "Cantidad") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust=1, vjust=0.4, size=6))
```

Vemos que efectivamente Palermo es el barrio donde más delitos se producen y aún más, desestimaremos  robo y hurto automotor porque son muy pocos los casos en comparación con los de hurto y robo. Como Palermo es el barrio con más delitos solo nos quedaremos con sus datos y además con los datos de Robo total y Hurto total.

```{r}
delitos <- delitos %>% filter(barrio == "PALERMO")
delitos <- delitos %>% filter(subtipo == "Robo total" | subtipo == "Hurto total")
```
Para evitar redundancia en los datos, hay varias columnas que podemos eliminar. Estas son subtipo (pues tipo ya lo define), barrio y comuna (porque todos los delitos ocurrieron en Palermo) y cantidad porque todas las filas tienen 1.

```{r}
delitos <- delitos %>% select(-subtipo, -barrio, -comuna, -cantidad)
```

Armamos un objeto espacial del dataset.
```{r}
delitos.sf = delitos %>% st_as_sf(coords = c("longitud", "latitud"), crs = 4326)
delitos.sf
```

Hagamos una visualización simple para ver si aporta información. 
```{r}
plot(delitos.sf)
```
Esto es si graficamos todo junto. No es lo mejor definitivamente. Empecemos a segmentar.
Graficar todos los delitos juntos mucha información vimos que no aporta pues todo Palermo representa una zona donde se roba. Aún más, los espacios en blanco corresponden con espacios verdes o zonas no accesibles para el público.

Corroboremos esto último graficando las calles del barrio.
```{r}
calles.comp <- st_read("./callejero/")
calles<-calles.comp[,c("id","tipo_c","nom_mapa","BARRIO", "long")]
```

```{r}
calles <- calles %>% filter(BARRIO == "PALERMO")
avenidas <- calles %>% filter(tipo_c == "AVENIDA")
```

```{r}
ggplot(calles) +
  geom_sf(size = 0.3) +
  theme_minimal()
```

```{r}
ggplot() +
  geom_sf(data = calles, color = "gray80", size = 0.3) +
  geom_sf(data = delitos.sf, aes(color = as.factor(anio)), size = 0.3) +
  scale_color_brewer(palette = "Set1", name = "Año") +
  theme_minimal()

```


```{r}
leaflet(delitos.sf) %>% addTiles() %>% addCircleMarkers(radius=0.2)
```
Con esto confirmamos lo que discutíamos arriba. Ahora segmentemos los delitos según año, tipo y ambos en simultáneo para poder ver si aparece alguna tendencia obvia.

Hagamos este mismo gráfico pero primero:
1. Separando por año
2. Segmentando por tipo de delito
3. Segmentando por año y tipo de delito

```{r}

pal <- colorFactor(palette = "Set1", domain = delitos.sf$anio)

leaflet(delitos.sf) %>%
  addTiles() %>%
  addCircleMarkers(radius = 0.5, color = ~pal(anio), stroke = FALSE, fillOpacity = 0.8) %>%
  addLegend("bottomright", pal = pal, values = ~anio, title = "Año")

```


Veamos si efectivamente como parece la mayor proporción de robos se dio en 2024 con un histograma

```{r}
delitos.sf %>%
  filter(tipo == "Robo") %>%
  ggplot(aes(x = as.factor(anio))) +
  geom_bar(fill = "steelblue") +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) + 
  labs(
    title = "Cantidad de Robos por Año",
    x = "Año",
    y = "Cantidad de Robos"
  ) 
```
Vemos que justamente no. Fue por un tema de las etiquetas. Ahora hagamos el mapa para los 3 años separados.

```{r}
ggplot(delitos.sf, aes(color = as.factor(anio))) +
  geom_sf(data = calles, color = "gray80", size = 0.3) +
  geom_sf(size = 0.3) +
  scale_color_brewer(palette = "Set1", name = "Año") +
  facet_wrap(~ anio) +
  theme_minimal() +
  guides(color = "none")  
```
Se ve evidentemente que los delitos ocurren en todo Palermo y no parece haber un cambio notable según el año.

Vamos a segmentar por tipo de robo pero de acuerdo al año.

```{r}

ggplot(delitos.sf, aes(x = tipo, fill = as.factor(uso_arma))) +
  geom_bar(position = "dodge") +
  facet_wrap(~ anio) +
  labs(
    x = "Tipo de delito",
    y = "Cantidad",
    fill = "Uso de arma",
    title = "Delitos por Año, Tipo y Uso de Arma"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
Vamos a segmentar por año, uso de arma, uso de moto y tipo.
```{r}
ggplot(delitos.sf, aes(x = as.factor(uso_moto), fill = as.factor(uso_arma))) +
  geom_bar(position = "dodge") +
  facet_grid(anio ~ tipo) +
  labs(
    x = "Uso de moto",
    y = "Cantidad",
    fill = "Uso de arma",
    title = "Delitos segmentados por Año, Tipo, Arma y Moto"
  ) +
  theme_minimal()
```

Vemos que aquellos delitos donde se usa moto o arma representan una menor proporción por lo tanto los desestimaremos.

```{r}
delitos.sf <- delitos.sf %>% select(-uso_moto)
delitos.sf <- delitos.sf %>% select(-uso_arma)
delitos <- delitos %>% select(-uso_moto)
delitos <- delitos %>% select(-uso_arma)
```

Grafiquemos para un mismo año un mapa que contenga los dos tipos de delitos.
```{r}
anios <- sort(unique(delitos.sf$anio))
mapas_por_anio <- map(anios, function(anio_actual) {
  datos <- delitos.sf %>% filter(anio == anio_actual)
  
  pal <- colorFactor(palette = "Set1", domain = datos$tipo)
  
  leaflet(datos) %>%
    addTiles() %>%
    addCircleMarkers(
      radius = 0.5,
      color = ~pal(tipo),
      stroke = FALSE,
      fillOpacity = 0.8
    ) %>%
    addLegend("bottomright", pal = pal, values = ~tipo,
              title = paste("Tipos de delito<br>Año:", anio_actual))
})

names(mapas_por_anio) <- as.character(anios)
```

```{r}
mapas_por_anio[["2022"]]

```

```{r}
mapas_por_anio[["2023"]]

```


```{r}
mapas_por_anio[["2024"]]

```

Verlo todo junto no es lo mejor, decidimos graficar por año lado a lado para poder comparar.
```{r}
delitos_filtrados <- delitos.sf %>%
  filter(tipo %in% c("Robo", "Hurto"),
         anio == 2022)

ggplot() +
  geom_sf(data = calles, color = "gray80", size = 0.3) +
  geom_sf(data = delitos_filtrados, size = 0.3) +
  facet_wrap(~ tipo, nrow = 1) +
  theme_minimal() + labs(title = "Mapa de delitos por tipo - Año 2022")
```

```{r}
delitos_filtrados <- delitos.sf %>%
  filter(tipo %in% c("Robo", "Hurto"),
         anio == 2023)

ggplot() +
  geom_sf(data = calles, color = "gray80", size = 0.3) +
  geom_sf(data = delitos_filtrados, size = 0.3) +
  facet_wrap(~ tipo, nrow = 1) +
  theme_minimal() + labs(title = "Mapa de delitos por tipo - Año 2023")
```

```{r}
delitos_filtrados <- delitos.sf %>%
  filter(tipo %in% c("Robo", "Hurto"),
         anio == 2024)

ggplot() +
  geom_sf(data = calles, color = "gray80", size = 0.3) +
  geom_sf(data = delitos_filtrados, size = 0.3) +
  facet_wrap(~ tipo, nrow = 1) +
  theme_minimal() + labs(title = "Mapa de delitos por tipo - Año 2024")
```
En los tres casos vemos que los mapas son similares, no parece haber una tendencia obvia. Palermo es peligroso en general.

Veamos si podemos obtener más información segmentando de acuerdo a franja horaria.

```{r}
anio_interes <- 2022

delitos.sf_franja <- delitos.sf
delitos.sf_franja$franja <- factor(delitos.sf_franja$franja, levels = 0:23)

delitos_anio <- delitos.sf_franja %>%
  filter(anio == anio_interes)


ggplot(delitos_anio, aes(x = franja)) +
  geom_bar(fill = "steelblue") +
  labs(
    title = paste("Cantidad de delitos por hora - Año", anio_interes),
    x = "Hora del día",
    y = "Cantidad de delitos"
  ) +
  theme_minimal()
```

```{r}
anio_interes <- 2023

delitos.sf_franja <- delitos.sf
delitos.sf_franja$franja <- factor(delitos.sf_franja$franja, levels = 0:23)

delitos_anio <- delitos.sf_franja %>%
  filter(anio == anio_interes)

ggplot(delitos_anio, aes(x = franja)) +
  geom_bar(fill = "steelblue") +
  labs(
    title = paste("Cantidad de delitos por hora - Año", anio_interes),
    x = "Hora del día",
    y = "Cantidad de delitos"
  ) +
  theme_minimal()
```

```{r}
anio_interes <- 2024

delitos.sf_franja <- delitos.sf
delitos.sf_franja$franja <- factor(delitos.sf_franja$franja, levels = 0:23)

delitos_anio <- delitos.sf_franja %>%
  filter(anio == anio_interes)

ggplot(delitos_anio, aes(x = franja)) +
  geom_bar(fill = "steelblue") +
  labs(
    title = paste("Cantidad de delitos por hora - Año", anio_interes),
    x = "Hora del día",
    y = "Cantidad de delitos"
  ) +
  theme_minimal()
```
Se suele robar más a la tarde/noche, teorizamos que es por el flujo de gente (gente que sale de trabajar, gente que sale a comer, etc.), sin embargo, la varianza no parece ser tan alta, es por eso que decidimos ignorar esto para el análisis que sigue.

Fijemonos si hay una tendencia mensual.

```{r}
anio_interes <- 2022

meses_orden <- c("ENERO", "FEBRERO", "MARZO", "ABRIL", "MAYO",
                 "JUNIO", "JULIO", "AGOSTO", "SEPTIEMBRE", "OCTUBRE",
                 "NOVIEMBRE", "DICIEMBRE")

delitos_anio <- delitos.sf %>%
  filter(anio == anio_interes) %>%
  mutate(
    mes = factor(mes, levels = meses_orden)
  )

ggplot(delitos_anio, aes(x = mes)) +
  geom_bar(fill = "steelblue") +
  labs(
    title = paste("Cantidad de delitos por mes - Año", anio_interes),
    x = "Mes",
    y = "Cantidad de delitos"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(size = 6),  
    axis.text.y = element_text(size = 6)   
  )
```

```{r}

anio_interes <- 2023

meses_orden <- c("ENERO", "FEBRERO", "MARZO", "ABRIL", "MAYO",
                 "JUNIO", "JULIO", "AGOSTO", "SEPTIEMBRE", "OCTUBRE",
                 "NOVIEMBRE", "DICIEMBRE")

delitos_anio <- delitos.sf %>%
  filter(anio == anio_interes) %>%
  mutate(
    mes = factor(mes, levels = meses_orden)
  )

ggplot(delitos_anio, aes(x = mes)) +
  geom_bar(fill = "steelblue") +
  labs(
    title = paste("Cantidad de delitos por mes - Año", anio_interes),
    x = "Mes",
    y = "Cantidad de delitos"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(size = 6),  
    axis.text.y = element_text(size = 6)   
  )
```

```{r}

anio_interes <- 2024


meses_orden <- c("ENERO", "FEBRERO", "MARZO", "ABRIL", "MAYO",
                 "JUNIO", "JULIO", "AGOSTO", "SEPTIEMBRE", "OCTUBRE",
                 "NOVIEMBRE", "DICIEMBRE")

delitos_anio <- delitos.sf %>%
  filter(anio == anio_interes) %>%
  mutate(
    mes = factor(mes, levels = meses_orden)
  )

ggplot(delitos_anio, aes(x = mes)) +
  geom_bar(fill = "steelblue") +
  labs(
    title = paste("Cantidad de delitos por mes - Año", anio_interes),
    x = "Mes",
    y = "Cantidad de delitos"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(size = 6),  
    axis.text.y = element_text(size = 6)   
  )
```
No hay una tendencia clara en los robos por mes. Sin embargo, mantendremos estas dos variables porque, quizás, al mirarlas en conjunto, podrían brindarnos información relevante.

Fijemonos si hay un hotspot espacial.
```{r}
delitos.sf_hs <- st_transform(delitos.sf, crs = 22185)
calles_hs <- st_transform(calles, crs = 22185)


coords_df <- delitos.sf_hs |>
  mutate(anio = as.factor(anio)) |>
  cbind(st_coordinates(delitos.sf_hs)) |>
  st_drop_geometry()



# Año 2022
gg_2022 <- ggplot() +
  geom_sf(data = calles_hs, color = "gray80", size = 0.2) +
  stat_density_2d(data = filter(coords_df, anio == "2022"),
                  aes(x = X, y = Y, fill = after_stat(level)),
                  geom = "polygon", alpha = 0.5) +
  scale_fill_viridis_c(name = "Densidad") +
  ggtitle("Hotspots - Año 2022") +
  theme_minimal() +
  coord_sf()

# Año 2023
gg_2023 <- ggplot() +
  geom_sf(data = calles_hs, color = "gray80", size = 0.2) +
  stat_density_2d(data = filter(coords_df, anio == "2023"),
                  aes(x = X, y = Y, fill = after_stat(level)),
                  geom = "polygon", alpha = 0.5) +
  scale_fill_viridis_c(name = "Densidad") +
  ggtitle("Hotspots - Año 2023") +
  theme_minimal() +
  coord_sf()

# Año 2024
gg_2024 <- ggplot() +
  geom_sf(data = calles_hs, color = "gray80", size = 0.2) +
  stat_density_2d(data = filter(coords_df, anio == "2024"),
                  aes(x = X, y = Y, fill = after_stat(level)),
                  geom = "polygon", alpha = 0.5) +
  scale_fill_viridis_c(name = "Densidad") +
  ggtitle("Hotspots - Año 2024") +
  theme_minimal() +
  coord_sf()

# Mostrarlos individualmente
gg_2022
gg_2023
gg_2024

```
Grafiquemos los hotspots de manera interactiva para ver qué zonas son las que marca el mapa de arriba.

Importemos unas librerías que serán utilizadas a partir de esta parte.

```{r}
library(purrr)
library(MASS)
library(raster)
library(caret)
library(pROC)
library(randomForest)
library(xgboost)
library(gridExtra)
library(spatstat)
```


```{r}
delitos_proj <- st_transform(delitos.sf, 22185)
calles_proj <- st_transform(calles, 22185)


coords_df <- delitos_proj |>
  mutate(anio = as.factor(anio)) |>
  cbind(st_coordinates(delitos_proj)) |>
  st_drop_geometry()
crear_mapa_leaflet <- function(df_anio, calles, titulo) {

  dens <- kde2d(df_anio$X, df_anio$Y, n = 200)
  
  r <- raster(list(x = dens$x, y = dens$y, z = dens$z))
  crs(r) <- CRS("+init=epsg:22185")  # EPSG del raster original
  r <- projectRaster(r, crs = CRS("+init=epsg:4326"))  # reproyectar a WGS84 para leaflet


  calles_wgs <- st_transform(calles, 4326)

  leaflet() |>
    addTiles() |>
    addRasterImage(r, colors = colorNumeric("viridis", values(r), na.color = NA), opacity = 0.5) |>
    addPolylines(data = calles_wgs, color = "gray", weight = 0.7) |>
    addLegend(pal = colorNumeric("viridis", values(r), na.color = NA),
              values = values(r), title = "Densidad", position = "bottomright") |>
    addControl(titulo, position = "topright")
}

```

```{r}
# Filtrar por año
df_2022 <- filter(coords_df, anio == "2022")
df_2023 <- filter(coords_df, anio == "2023")
df_2024 <- filter(coords_df, anio == "2024")

# Generar mapas
mapa_2022 <- crear_mapa_leaflet(df_2022, calles_proj, "Hotspots 2022")
mapa_2023 <- crear_mapa_leaflet(df_2023, calles_proj, "Hotspots 2023")
mapa_2024 <- crear_mapa_leaflet(df_2024, calles_proj, "Hotspots 2024")

# Mostrar (uno a la vez en RStudio Viewer)
mapa_2022
mapa_2023
mapa_2024

```

Vemos que los hotspots fueron variando a lo largo de los años pero que de 2022 a 2023 hay una gran diferencia. Por ello, si se busca armar un modelo predictivo se debe descartar aprender del 2022 puesto que va a añadir cierto sesgo al modelo y creemos que puede ser perjudicial. 

Nosotros ahora armaremos diferentes modelos y para ello entrenaremos con todo el 2023 y luego predeciremos los valores de 2024. A la hora de hacer modelos con datos temporales es muy importante no mezclar la fecha de los sucesos.

Lo primero que hacemos es transformar el problema en uno de clasificación binaria.Las instancias positivas serán los robos y las instancias negativas serán los hurtos

```{r}
delitos_filtrado <- delitos.sf %>% 
  filter(anio != 2022)


meses_ordenados <- c("ENERO", "FEBRERO", "MARZO", "ABRIL", "MAYO", "JUNIO",
                     "JULIO", "AGOSTO", "SEPTIEMBRE", "OCTUBRE", "NOVIEMBRE", "DICIEMBRE")

delitos_filtrado <- delitos_filtrado %>%
  mutate(mes_num = match(mes, meses_ordenados))

delitos_filtrado <- delitos_filtrado %>%
  mutate(res = ifelse(tipo=="Hurto", 0, 1))

preTrain <- delitos_filtrado %>%
  filter(anio == 2023 | (anio == 2024 & mes_num <= 9))

Train <- preTrain  %>%
  filter(anio == 2023 | (anio == 2024 & mes_num <= 6))

Test <- preTrain %>%
  filter(anio == 2024 & mes_num <= 9 & mes_num > 6 )

Validation <- delitos_filtrado %>%
  filter(anio == 2024 & mes_num >= 10)

```

Para hacer el análisis del riesgo relativo, filtramos el dataset de la misma forma, pues luego vamos a querer compararlo con los modelos que entrenemos.
```{r}
barrios_caba <- st_read("http://cdn.buenosaires.gob.ar/datosabiertos/datasets/barrios/barrios.geojson") %>%
  filter(BARRIO == "PALERMO")

palermo_sf <- barrios_caba$geometry[1]
```
```{r}
library(spatstat)
```

```{r}
dataset_juguete <- delitos_filtrado %>% filter(res==1)
primeras_n_filas <- delitos_filtrado %>% filter(res==0) %>% slice_head(n=100)
dataset_juguete <- bind_rows(dataset_juguete, primeras_n_filas)
```


```{r}
# Reproyectar al CRS plano
delitos_proj <- st_transform(delitos_filtrado, 32721)
palermo_proj <- st_transform(palermo_sf, 32721)
#delitos_proj <- st_transform(dataset_juguete, 32721)

# Filtrar solo puntos dentro del polígono
inside_idx <- st_within(delitos_proj, palermo_proj, sparse = FALSE)[,1]
delitos_dentro <- delitos_proj[inside_idx, ]

# Crear ventana
win <- as.owin(palermo_proj)

# --- CREAR PPP solo con geometry + res ---
# extraer coords
coords <- st_coordinates(delitos_dentro)

# construir ppp
X <- ppp(
  x = coords[,1],
  y = coords[,2],
  window = win,
  marks = factor(delitos_dentro$res, labels = c("NoDelito","Delito"))
)

# Riesgo relativo
rr  <- relrisk(X, relative = TRUE)
rrF <- relrisk(X, relative = FALSE)
plot(rr, main = "Riesgo Relativo en Palermo")
points(X, pch=20, cex=0.4)

```

```{r}
#DS Juguete
plot(rr, main = "Riesgo Relativo en Palermo")
plot(rrF, main = "Riesgo Relativo en Palermo")
```


```{r}
plot(rr, main = "Riesgo Relativo en Palermo") #DS Completo
plot(rrF, main = "Riesgo Relativo en Palermo")
```


Entrenaremos 3 modelos muy utilizados para clasificación binaria: regresión logística, Random Forest y XGBoost.



Primero, vamos a fitear 3 modelos dejando que solo utilicen latitud y longitud como variables explicativas.
```{r}
delitos_filtrado_solo_lat_long <- delitos %>% 
  filter(anio != 2022)

meses_ordenados <- c("ENERO", "FEBRERO", "MARZO", "ABRIL", "MAYO", "JUNIO",
                     "JULIO", "AGOSTO", "SEPTIEMBRE", "OCTUBRE", "NOVIEMBRE", "DICIEMBRE")

delitos_filtrado_solo_lat_long <- delitos_filtrado_solo_lat_long %>%
  mutate(mes_num = match(mes, meses_ordenados))

delitos_filtrado_solo_lat_long$tipo <- ifelse(delitos_filtrado_solo_lat_long$tipo == "Hurto", 1, 0)

delitos_filtrado_solo_lat_long <- delitos_filtrado_solo_lat_long %>%
  mutate(res = ifelse(tipo=="Hurto", 0, 1))



preTrain <- delitos_filtrado_solo_lat_long %>%
  filter(anio == 2023 | (anio == 2024 & mes_num <= 9))

Train <- preTrain  %>%
  filter(anio == 2023 | (anio == 2024 & mes_num <= 6))

Test <- preTrain %>%
  filter(anio == 2024 & mes_num <= 9 & mes_num > 6 )

Validation <- delitos_filtrado_solo_lat_long %>%
  filter(anio == 2024 & mes_num >= 10)


```

```{r}
Train$latitud <- as.numeric(Train$latitud)
Train$longitud <- as.numeric(Train$longitud)

Test$latitud <- as.numeric(Test$latitud)
Test$longitud <- as.numeric(Test$longitud)

Validation$latitud <- as.numeric(Validation$latitud)
Validation$longitud <- as.numeric(Validation$longitud)

preTrain$latitud <- as.numeric(preTrain$latitud)
preTrain$longitud <- as.numeric(preTrain$longitud)


```

```{r}
install.packages("pROC")
```
Definimos ciertas funciones que van a ayudar e importamos librerías.

```{r}
library(pROC)
library(sf)
library(ggplot2)
library(dplyr)
library(gridExtra)
library(randomForest)
library(xgboost)


calcular_metricas <- function(real, predicho, probabilidades, nombre_conjunto) {
  tabla <- table(Observado = real, Predicho = predicho)
  
  accuracy <- sum(diag(tabla)) / sum(tabla)
  precision <- ifelse(sum(tabla[,2]) > 0, tabla[2,2] / sum(tabla[,2]), 0)
  recall <- ifelse(sum(tabla[2,]) > 0, tabla[2,2] / sum(tabla[2,]), 0)
  f1 <- ifelse((precision + recall) > 0, 2 * precision * recall / (precision + recall), 0)
  roc_obj <- roc(real, probabilidades, quiet = TRUE)
  auc_value <- auc(roc_obj)
  plot(roc_obj, 
     main = paste("Curva ROC en", nombre_conjunto) , 
     col = "blue", 
     lwd = 2,
     print.auc = TRUE)
  
  cat("=== MÉTRICAS -", nombre_conjunto, "===\n")
  print(tabla)
  cat("Accuracy:", round(accuracy, 3), "\n")
  cat("Precision:", round(precision, 3), "\n")
  cat("Recall:", round(recall, 3), "\n")
  cat("F1-Score:", round(f1, 3), "\n\n")
  cat("AUC-ROC con pROC:", round(auc_value, 3), "\n")
  
  return(list(accuracy = accuracy, precision = precision, recall = recall, f1 = f1))
}

# Función para convertir dataframe a shapefile y graficar
crear_shapefile <- function(datos, predicciones_prob, predicciones_class, nombre_conjunto) {
  # Crear dataframe con todas las variables
  datos_sf <- datos %>%
    mutate(
      pred_prob = predicciones_prob,
      pred_class = predicciones_class,
      conjunto = nombre_conjunto,
      tipo_real = factor(tipo, levels = c(0, 1), labels = c("Hurto", "Robo")),
      tipo_pred = factor(pred_class, levels = c(0, 1), labels = c("Hurto", "Robo")),
      correcta = ifelse(tipo == pred_class, "Correcta", "Incorrecta")
    )
  
  # Convertir a objeto sf (shapefile)
  datos_sf <- st_as_sf(datos_sf, coords = c("longitud", "latitud"), crs = 4326)
  
  return(datos_sf)
}

# Creamos mapas comparativos

crear_mapa_comparativo <- function(shapefile_data, titulo_conjunto) {
  
  # Extraer coordenadas para ggplot
  coords <- st_coordinates(shapefile_data)
  plot_data <- cbind(st_drop_geometry(shapefile_data), coords)
  
  # Mapa 1: Valores reales
  mapa_real <- ggplot(plot_data, aes(x = X, y = Y)) +
    geom_point(aes(color = tipo_real), alpha = 0.7, size = 0.8) +
    scale_color_manual(values = c("Hurto" = "blue", "Robo" = "red"), 
                       name = "Tipo Real") +
    theme_minimal() +
    theme(axis.text = element_text(size = 8),
          legend.title = element_text(size = 9),
          legend.text = element_text(size = 8)) +
    labs(title = paste("Valores Reales -", titulo_conjunto),
         x = "Longitud", y = "Latitud")
  
  # Mapa 2: Predicciones (probabilidades)
  mapa_prob <- ggplot(plot_data, aes(x = X, y = Y)) +
    geom_point(aes(color = pred_prob), alpha = 0.7, size = 0.8) +
    scale_color_gradient2(low = "blue", mid = "white", high = "red", 
                         midpoint = 0.5, name = "P(Robo)") +
    theme_minimal() +
    theme(axis.text = element_text(size = 8),
          legend.title = element_text(size = 9),
          legend.text = element_text(size = 8)) +
    labs(title = paste("Probabilidades Predichas -", titulo_conjunto),
         x = "Longitud", y = "Latitud")
  
  # Mapa 3: Predicciones (clases)
  mapa_pred <- ggplot(plot_data, aes(x = X, y = Y)) +
    geom_point(aes(color = tipo_pred), alpha = 0.7, size = 0.8) +
    scale_color_manual(values = c("Hurto" = "blue", "Robo" = "red"), 
                       name = "Tipo Pred.") +
    theme_minimal() +
    theme(axis.text = element_text(size = 8),
          legend.title = element_text(size = 9),
          legend.text = element_text(size = 8)) +
    labs(title = paste("Clases Predichas -", titulo_conjunto),
         x = "Longitud", y = "Latitud")
  
  # Mapa 4: Correctas vs Incorrectas
  mapa_accuracy <- ggplot(plot_data, aes(x = X, y = Y)) +
    geom_point(aes(color = correcta), alpha = 0.7, size = 0.8) +
    scale_color_manual(values = c("Correcta" = "green", "Incorrecta" = "orange"), 
                       name = "Predicción") +
    theme_minimal() +
    theme(axis.text = element_text(size = 8),
          legend.title = element_text(size = 9),
          legend.text = element_text(size = 8)) +
    labs(title = paste("Precisión Espacial -", titulo_conjunto),
         x = "Longitud", y = "Latitud")
  
  return(list(real = mapa_real, prob = mapa_prob, pred = mapa_pred, accuracy = mapa_accuracy))
}
```


```{r}
# Definimos cantidad de árboles a probar
ntrees_valores <- c(50, 100, 200, 300, 400, 500)


modelos_rf <- list()
metricas_por_ntrees <- data.frame()
errores_oob <- data.frame()
predicciones_por_modelo <- list()

set.seed(123)  # Para reproducibilidad


# Entrenamos 
for (i in seq_along(ntrees_valores)) {
  ntree_actual <- ntrees_valores[i]
  
  
  # Entrenar modelo
  modelo <- randomForest(factor(tipo) ~ latitud + longitud, 
                        data = Train,
                        ntree = ntree_actual,
                        mtry = 2,
                        importance = TRUE,
                        do.trace = FALSE)
  
  modelos_rf[[as.character(ntree_actual)]] <- modelo
  
  # Almacenar error OOB
  errores_oob <- rbind(errores_oob,
                      data.frame(ntrees = ntree_actual,
                                error_oob = modelo$err.rate[ntree_actual, "OOB"]))
  
}

# Evaluamos cada modelo
for (i in seq_along(ntrees_valores)) {
  ntree_actual <- ntrees_valores[i]
  modelo <- modelos_rf[[as.character(ntree_actual)]]
  
  # Predicciones en Test
  pred_prob <- predict(modelo, Test, type = "prob")[,2]
  pred_class <- as.numeric(as.character(predict(modelo, Test, type = "class")))
  
  # Calcular métricas
  tabla_confusion <- table(Observado = Test$tipo, Predicho = pred_class)
  
  accuracy <- sum(diag(tabla_confusion)) / sum(tabla_confusion)
  precision <- ifelse(sum(tabla_confusion[,2]) > 0, 
                     tabla_confusion[2,2] / sum(tabla_confusion[,2]), 0)
  recall <- ifelse(sum(tabla_confusion[2,]) > 0, 
                  tabla_confusion[2,2] / sum(tabla_confusion[2,]), 0)
  f1 <- ifelse((precision + recall) > 0, 
              2 * precision * recall / (precision + recall), 0)
  roc_obj <- roc(Test$tipo, pred_prob, quiet = TRUE)
  auc_value <- auc(roc_obj)
  # Almacenar métricas
  metricas_por_ntrees <- rbind(metricas_por_ntrees,
                              data.frame(ntrees = ntree_actual,
                                        accuracy = accuracy,
                                        precision = precision,
                                        recall = recall,
                                        f1_score = f1,
                                        auc_roc = auc_value))
  
  # Almacenar predicciones para análisis posterior
  predicciones_por_modelo[[as.character(ntree_actual)]] <- list(
    prob = pred_prob,
    class = pred_class
  )
}


# Graficamos el error OOB
grafico_oob <- ggplot(errores_oob, aes(x = ntrees, y = error_oob)) +
  geom_line(color = "red", size = 1.2) +
  geom_point(color = "darkred", size = 3) +
  geom_text(aes(label = round(error_oob, 3)), 
            vjust = -0.5, hjust = 0.5, size = 3) +
  theme_minimal() +
  labs(title = "Error Out-of-Bag vs Número de Árboles",
       x = "Número de Árboles",
       y = "Error OOB") +
  scale_x_continuous(breaks = ntrees_valores)

# Graficamos métricas
metricas_long <- metricas_por_ntrees %>%
  tidyr::pivot_longer(cols = c(accuracy, precision, recall, f1_score),
                      names_to = "metrica", values_to = "valor")

grafico_metricas <- ggplot(metricas_long, aes(x = ntrees, y = valor, color = metrica)) +
  geom_line(size = 1.2) +
  geom_point(size = 3) +
  theme_minimal() +
  labs(title = "Métricas en Conjunto Test vs Número de Árboles",
       x = "Número de Árboles",
       y = "Valor de la Métrica",
       color = "Métrica") +
  scale_x_continuous(breaks = ntrees_valores) +
  scale_y_continuous(limits = c(0, 1)) +
  theme(legend.position = "bottom")


# Mostrar gráficos

print(grafico_oob)
print(grafico_metricas)


# Hacemos una tabla resumen

tabla_resumen <- merge(tiempos_entrenamiento, errores_oob, by = "ntrees")
tabla_resumen <- merge(tabla_resumen, metricas_por_ntrees, by = "ntrees")

# Formatear tabla para mostrar
tabla_mostrar <- tabla_resumen %>%
  mutate(
  
    error_oob = round(error_oob, 4),
    accuracy = round(accuracy, 3),
    precision = round(precision, 3),
    recall = round(recall, 3),
    f1_score = round(f1_score, 3),
    auc_roc = round(auc_roc, 3)
  ) %>%
  dplyr::select(ntrees, error_oob, accuracy, precision, recall, f1_score, auc_roc)

print(tabla_mostrar)


# Recomendar número óptimo de árboles
mejor_f1_idx <- which.max(metricas_por_ntrees$f1_score)
mejor_ntrees_f1 <- ntrees_valores[mejor_f1_idx]
mejor_f1 <- max(metricas_por_ntrees$f1_score)

mejor_accuracy_idx <- which.max(metricas_por_ntrees$accuracy)
mejor_ntrees_accuracy <- ntrees_valores[mejor_accuracy_idx]
mejor_accuracy <- max(metricas_por_ntrees$accuracy)

mejor_aucroc_idx <- which.max(metricas_por_ntrees$auc_roc)
mejor_ntrees_aucroc <- ntrees_valores[mejor_aucroc_idx]
mejor_aucroc <- max(metricas_por_ntrees$auc_roc)


cat("Mejor F1-Score:", mejor_ntrees_f1, "árboles (F1 =", round(mejor_f1, 3), ")\n")
cat("Mejor Accuracy:", mejor_ntrees_accuracy, "árboles (Acc =", round(mejor_accuracy, 3), ")\n")
cat("Mejor AUC-ROC:", mejor_ntrees_aucroc, "árboles (F1 =", round(mejor_aucroc, 3), ")\n")
```



```{r}


# Definimos los mejores hiperparámetros
# Definir grid de parámetros
nrounds_grid <- seq(50, 100, by = 10)  # 50, 60, 70, 80, 90, 100
eta_grid <- c(0.1, 0.2, 0.3)
max_depth_fijo <- 6

# Crear grid completo de combinaciones
param_grid <- expand.grid(
  nrounds = nrounds_grid,
  eta = eta_grid,
  max_depth = max_depth_fijo,
  stringsAsFactors = FALSE
)

# Preparar datos para XGBoost
dtrain <- xgb.DMatrix(data = as.matrix(Train[, c("latitud", "longitud")]), 
                      label = Train$tipo)
dtest <- xgb.DMatrix(data = as.matrix(Test[, c("latitud", "longitud")]), 
                     label = Test$tipo)

dpreTrain <- xgb.DMatrix(data = as.matrix(preTrain[, c("latitud", "longitud")]), 
                      label = preTrain$tipo)
dvalidation <- xgb.DMatrix(data = as.matrix(Validation[, c("latitud", "longitud")]), 
                          label = Validation$tipo)

# Inicializar estructuras para almacenar resultados
modelos_xgb <- list()
resultados_grid <- data.frame()
predicciones_por_modelo <- list()

set.seed(123)  # Para reproducibilidad

# Entrenamos modelos

for (i in 1:nrow(param_grid)) {
  params <- param_grid[i, ]
  nrounds_actual <- params$nrounds
  eta_actual <- params$eta
  max_depth_actual <- params$max_depth
  
  combo_name <- paste0("nr", nrounds_actual, "_eta", eta_actual, "_md", max_depth_actual)

 
  
  # Parámetros del modelo
  params_xgb <- list(
    objective = "binary:logistic",
    eval_metric = "logloss",
    eta = eta_actual,
    max_depth = max_depth_actual,
    subsample = 0.8,
    colsample_bytree = 1.0,
    seed = 123
  )
  
  # Entrenar modelo
  modelo <- xgb.train(
    params = params_xgb,
    data = dtrain,
    nrounds = nrounds_actual,
    watchlist = list(train = dtrain, eval = dtest),
    verbose = 0,
    early_stopping_rounds = 10
  )
  
 
  
  # Almacenar modelo
  modelos_xgb[[combo_name]] <- modelo
  
  # Hacer predicciones en Test
  pred_prob <- predict(modelo, dtest)
  pred_class <- ifelse(pred_prob > 0.5, 1, 0)
  
  # Calcular métricas
  tabla_confusion <- table(Observado = Test$tipo, Predicho = pred_class)
  
  accuracy <- sum(diag(tabla_confusion)) / sum(tabla_confusion)
  precision <- ifelse(sum(tabla_confusion[,2]) > 0, 
                     tabla_confusion[2,2] / sum(tabla_confusion[,2]), 0)
  recall <- ifelse(sum(tabla_confusion[2,]) > 0, 
                  tabla_confusion[2,2] / sum(tabla_confusion[2,]), 0)
  f1 <- ifelse((precision + recall) > 0, 
              2 * precision * recall / (precision + recall), 0)
  
  roc_obj <- roc(Test$tipo, pred_prob, quiet = TRUE)
  auc_value <- auc(roc_obj)
  # Obtener error de entrenamiento final
  train_error <- modelo$evaluation_log$train_logloss[nrow(modelo$evaluation_log)]
  eval_error <- modelo$evaluation_log$eval_logloss[nrow(modelo$evaluation_log)]
  
  # Almacenar resultados
  resultados_grid <- rbind(resultados_grid, data.frame(
    combo = combo_name,
    nrounds = nrounds_actual,
    eta = eta_actual,
    max_depth = max_depth_actual,
    tiempo_seg = tiempo_transcurrido,
    train_logloss = train_error,
    eval_logloss = eval_error,
    accuracy = accuracy,
    precision = precision,
    recall = recall,
    f1_score = f1,
    auc_roc = auc_value,
    stringsAsFactors = FALSE
  ))
  
  # Almacenar predicciones para análisis posterior
  predicciones_por_modelo[[combo_name]] <- list(
    prob = pred_prob,
    class = pred_class
  )

}



# Encontrar mejores modelos
mejor_f1_idx <- which.max(resultados_grid$f1_score)
mejor_accuracy_idx <- which.max(resultados_grid$accuracy)
mejor_precision_idx <- which.max(resultados_grid$precision)
mejor_recall_idx <- which.max(resultados_grid$recall)
mejor_aucroc_idx <- which.max(resultados_grid$auc_roc)

cat("Mejor F1-Score:", round(resultados_grid$f1_score[mejor_f1_idx], 3), 
    "| Combo:", resultados_grid$combo[mejor_f1_idx], "\n")
cat("Mejor Accuracy:", round(resultados_grid$accuracy[mejor_accuracy_idx], 3), 
    "| Combo:", resultados_grid$combo[mejor_accuracy_idx], "\n")
cat("Mejor Precision:", round(resultados_grid$precision[mejor_precision_idx], 3), 
    "| Combo:", resultados_grid$combo[mejor_precision_idx], "\n")
cat("Mejor Recall:", round(resultados_grid$recall[mejor_recall_idx], 3), 
    "| Combo:", resultados_grid$combo[mejor_recall_idx], "\n")
cat("Mejor AUC-ROC:", round(resultados_grid$auc_roc[mejor_aucroc_idx], 3), 
    "| Combo:", resultados_grid$combo[mejor_aucroc_idx], "\n\n")


# Seleccionar top 3 modelos
mejor_modelo <- resultados_grid[order(resultados_grid$auc_roc, decreasing = TRUE)[1], ]
combo_name <- mejor_modelo$combo
  
# Obtener predicciones
pred_prob <- predicciones_por_modelo[[combo_name]]$prob
pred_class <- predicciones_por_modelo[[combo_name]]$class
  
# Crear shapefile para Test
datos_sf <- Test %>%
mutate(
  pred_prob_xgb = pred_prob,
  pred_class_xgb = pred_class,
  combo = combo_name,
  tipo_real = factor(tipo, levels = c(0, 1), labels = c("Hurto", "Robo")),
  tipo_pred_xgb = factor(pred_class_xgb, levels = c(0, 1), labels = c("Hurto", "Robo")),
  correcta_xgb = ifelse(tipo == pred_class_xgb, "Correcta", "Incorrecta")
) %>%
st_as_sf(coords = c("longitud", "latitud"), crs = 4326)

pred_data <- data.frame(
  longitud = Test$longitud,
  latitud = Test$latitud,
  prob_robo = predicciones_por_modelo[[combo_name]]$prob,
  tipo_real = factor(Test$tipo, levels = c(0, 1), labels = c("Hurto", "Robo"))
)


tabla_resumen_xgb <- resultados_grid %>%
  mutate(
    eval_logloss = round(eval_logloss, 4),
    accuracy = round(accuracy, 3),
    precision = round(precision, 3),
    recall = round(recall, 3),
    f1_score = round(f1_score, 3),
    auc_roc = round(auc_roc, 4)
  ) %>%
  arrange(desc(auc_roc))

print(tabla_resumen_xgb)

mejor_modelo <- resultados_grid[mejor_aucroc_idx, ]

cat("Combinación:", mejor_modelo$combo, "\n")
cat("nrounds =", mejor_modelo$nrounds, "| eta =", mejor_modelo$eta, 
    "| max_depth =", mejor_modelo$max_depth, "\n")
cat("F1-Score:", round(mejor_modelo$f1_score, 3), "\n")
cat("AUC-ROC:", round(mejor_modelo$auc_roc, 3), "\n")



```


Ahora que conocemos las mejores configuracioens de XGBoost y RF, comparemos los resultados de los tres modelos entrenados.

Regresión logística

```{r}
# Entrenar el modelo de regresión logística
modelo_logistico <- glm(tipo ~ latitud + longitud, 
                       data = Train, 
                       family = binomial())

# Predecimos las respuestas
pred_train_prob <- predict(modelo_logistico, Train, type = "response")
pred_test_prob <- predict(modelo_logistico, Test, type = "response")

# Binarizamos
pred_train_class <- ifelse(pred_train_prob > 0.5, 1, 0)
pred_test_class <- ifelse(pred_test_prob > 0.5, 1, 0)


# Evaluar en todos los conjuntos
metricas_train <- calcular_metricas(Train$tipo, pred_train_class, pred_train_prob, "TRAIN")
metricas_test <- calcular_metricas(Test$tipo, pred_test_class,pred_test_prob ,"TEST")

shapefile_train <- crear_shapefile(Train, pred_train_prob, pred_train_class, "Train")
shapefile_test <- crear_shapefile(Test, pred_test_prob, pred_test_class, "Test")

mapas_train <- crear_mapa_comparativo(shapefile_train, "TRAIN")
mapas_test <- crear_mapa_comparativo(shapefile_test, "TEST")


grid.arrange(mapas_train$real, mapas_train$pred, ncol = 2)
grid.arrange(mapas_test$real, mapas_test$pred, ncol = 2)
grid.arrange(mapas_train$prob, mapas_train$accuracy, ncol = 2)
grid.arrange(mapas_test$prob, mapas_test$accuracy, ncol = 2)


# Estadísticas del Train
cat("=== ESTADÍSTICAS SHAPEFILE TRAIN ===\n")
cat("Total puntos:", nrow(shapefile_train), "\n")
cat("Predicciones correctas:", sum(shapefile_train$correcta == "Correcta"), "\n")
cat("Accuracy:", round(mean(shapefile_train$correcta == "Correcta"), 3), "\n")
cat("Distribución real - Hurtos:", sum(shapefile_train$tipo == 0), "| Robos:", sum(shapefile_train$tipo == 1), "\n")
cat("Distribución pred - Hurtos:", sum(shapefile_train$pred_class == 0), "| Robos:", sum(shapefile_train$pred_class == 1), "\n\n")

# Estadísticas del Test
cat("=== ESTADÍSTICAS SHAPEFILE TEST ===\n")
cat("Total puntos:", nrow(shapefile_test), "\n")
cat("Predicciones correctas:", sum(shapefile_test$correcta == "Correcta"), "\n")
cat("Accuracy:", round(mean(shapefile_test$correcta == "Correcta"), 3), "\n")
cat("Distribución real - Hurtos:", sum(shapefile_test$tipo == 0), "| Robos:", sum(shapefile_test$tipo == 1), "\n")
cat("Distribución pred - Hurtos:", sum(shapefile_test$pred_class == 0), "| Robos:", sum(shapefile_test$pred_class == 1), "\n\n")
```

```{r}
modelo <- randomForest(factor(tipo) ~ latitud + longitud, 
                        data = Train,
                        ntree = 100,
                        mtry = 2,
                        importance = TRUE,
                        do.trace = FALSE)

pred_train_prob<- predict(modelo, Train, type = "prob")[,2]
pred_train_class <- as.numeric(as.character(predict(modelo, Train, type = "class")))

pred_test_prob <- predict(modelo, Test, type = "prob")[,2]
pred_test_class <- as.numeric(as.character(predict(modelo, Test, type = "class")))

shapefile_train <- crear_shapefile(Train, pred_train_prob, pred_train_class, "Train")
shapefile_test <- crear_shapefile(Test, pred_test_prob, pred_test_class, "Test")


mapas_train <- crear_mapa_comparativo(shapefile_train, "TRAIN")
mapas_test <- crear_mapa_comparativo(shapefile_test, "TEST")



plot(roc(Train$tipo, pred_train_prob, quiet = TRUE), 
     main = paste("Curva ROC en Train") , 
     col = "blue", 
     lwd = 2,
     print.auc = TRUE)

plot(roc(Test$tipo, pred_test_prob, quiet = TRUE), 
   main = paste("Curva ROC en Test") , 
   col = "blue", 
   lwd = 2,
   print.auc = TRUE)

grid.arrange(mapas_train$real, mapas_train$pred, ncol = 2)
grid.arrange(mapas_test$real, mapas_test$pred, ncol = 2)
grid.arrange(mapas_train$prob, mapas_train$accuracy, ncol = 2)
grid.arrange(mapas_test$prob, mapas_test$accuracy, ncol = 2)
```

```{r}
params_xgb <- list(
    objective = "binary:logistic",
    eval_metric = "logloss",
    eta = 0.2,
    max_depth = 6,
    subsample = 0.8,
    colsample_bytree = 1.0,
    seed = 123
  )

modelo <- xgb.train(
  params = params_xgb,
  data = dtrain,
  nrounds = 60,
  watchlist = list(train = dtrain, eval = dtest),
  verbose = 0,
  early_stopping_rounds = 10
)


pred_train_prob <- predict(modelo, dtrain)
pred_train_class <- ifelse(pred_train_prob > 0.5, 1, 0)

pred_test_prob <- predict(modelo, dtest)
pred_test_class <- ifelse(pred_test_prob > 0.5, 1, 0)



shapefile_train <- crear_shapefile(Train, pred_train_prob, pred_train_class, "Train")
shapefile_test <- crear_shapefile(Test, pred_test_prob, pred_test_class, "Test")


mapas_train <- crear_mapa_comparativo(shapefile_train, "TRAIN")
mapas_test <- crear_mapa_comparativo(shapefile_test, "TEST")


plot(roc(Train$tipo, pred_train_prob, quiet = TRUE), 
     main = paste("Curva ROC en Train") , 
     col = "blue", 
     lwd = 2,
     print.auc = TRUE)

plot(roc(Test$tipo, pred_test_prob, quiet = TRUE), 
   main = paste("Curva ROC en Test") , 
   col = "blue", 
   lwd = 2,
   print.auc = TRUE)

grid.arrange(mapas_train$real, mapas_train$pred, ncol = 2)
grid.arrange(mapas_test$real, mapas_test$pred, ncol = 2)
grid.arrange(mapas_train$prob, mapas_train$accuracy, ncol = 2)
grid.arrange(mapas_test$prob, mapas_test$accuracy, ncol = 2)

```


Ahora que tenemos definidas las mejores configuraciones, determinemos con validación cuál es el mejor de estos tres modelos. Para determinar esto utilizaremos el AUC-ROC.

```{r}
modelo_logistico <- glm(tipo ~ latitud + longitud, 
                       data = preTrain, 
                       family = binomial())

# Predecimos las respuestas
pred_Validation_prob <- predict(modelo_logistico, Validation, type = "response")

# Binarizamos
pred_Validation_class <- ifelse(pred_Validation_prob > 0.5, 1, 0)


# Evaluar en todos los conjuntos
metricas_Validation <- calcular_metricas(Validation$tipo, pred_Validation_class,pred_Validation_prob ,"Validation")

shapefile_Validation <- crear_shapefile(Validation, pred_Validation_prob, pred_Validation_class, "Validation")

mapas_Validation <- crear_mapa_comparativo(shapefile_Validation, "VALIDATION")

grid.arrange(mapas_Validation$real, mapas_Validation$pred, ncol = 2)
grid.arrange(mapas_Validation$prob, mapas_Validation$accuracy, ncol = 2)

# Estadísticas del Test
cat("Total puntos:", nrow(shapefile_Validation), "\n")
cat("Predicciones correctas:", sum(shapefile_Validation$correcta == "Correcta"), "\n")
cat("Accuracy:", round(mean(shapefile_Validation$correcta == "Correcta"), 3), "\n")
cat("Distribución real - Hurtos:", sum(shapefile_Validation$tipo == 0), "| Robos:", sum(shapefile_Validation$tipo == 1), "\n")
cat("Distribución pred - Hurtos:", sum(shapefile_Validation$pred_class == 0), "| Robos:", sum(shapefile_Validation$pred_class == 1), "\n\n")
```


```{r}
modelo <- randomForest(factor(tipo) ~ latitud + longitud, 
                        data = preTrain,
                        ntree = 100,
                        mtry = 2,
                        importance = TRUE,
                        do.trace = FALSE)

pred_Validation_prob <- predict(modelo, Validation, type = "prob")[,2]
pred_Validation_class <- as.numeric(as.character(predict(modelo, Validation, type = "class")))
shapefile_Validation <- crear_shapefile(Validation, pred_Validation_prob, pred_Validation_class, "VALIDATION")
mapas_Validation <- crear_mapa_comparativo(shapefile_Validation, "VALIDATION")

plot(roc(Validation$tipo, pred_Validation_prob, quiet = TRUE), 
   main = paste("Curva ROC en VALIDATION") , 
   col = "blue", 
   lwd = 2,
   print.auc = TRUE)

grid.arrange(mapas_Validation$real, mapas_Validation$pred, ncol = 2)
grid.arrange(mapas_Validation$prob, mapas_Validation$accuracy, ncol = 2)
```

```{r}

params_xgb <- list(
    objective = "binary:logistic",
    eval_metric = "logloss",
    eta = 0.2,
    max_depth = 6,
    subsample = 0.8,
    colsample_bytree = 1.0,
    seed = 123
  )

modelo <- xgb.train(
  params = params_xgb,
  data = dpreTrain,
  nrounds = 60,
  watchlist = list(train = dpreTrain, eval = dvalidation),
  verbose = 0,
  early_stopping_rounds = 10
)

pred_validation_prob <- predict(modelo, dvalidation)
pred_validation_class <- ifelse(pred_validation_prob > 0.5, 1, 0)
shapefile_validation <- crear_shapefile(Validation, pred_validation_prob, pred_validation_class, "Validation")
mapas_test <- crear_mapa_comparativo(shapefile_validation, "VALIDATION")

plot(roc(Validation$tipo, pred_validation_prob, quiet = TRUE), 
   main = paste("Curva ROC en Validation") , 
   col = "blue", 
   lwd = 2,
   print.auc = TRUE)

grid.arrange(mapas_test$real, mapas_test$pred, ncol = 2)
grid.arrange(mapas_test$prob, mapas_test$accuracy, ncol = 2)
```


Evaluemos cuánto mejora la performance de los modelos si los entrenamos con todas las variables disponibles.

```{r}
# Definimos cantidad de árboles a probar
ntrees_valores <- c(50, 100, 200, 300, 400, 500)


modelos_rf <- list()
metricas_por_ntrees <- data.frame()
errores_oob <- data.frame()
predicciones_por_modelo <- list()

set.seed(123)  # Para reproducibilidad


# Entrenamos 
for (i in seq_along(ntrees_valores)) {
  ntree_actual <- ntrees_valores[i]
  
  
  # Entrenar modelo
  modelo <- randomForest(factor(tipo) ~ ., 
                        data = Train,
                        ntree = ntree_actual,
                        mtry = 2,
                        importance = TRUE,
                        do.trace = FALSE)
  
  modelos_rf[[as.character(ntree_actual)]] <- modelo
  
  # Almacenar error OOB
  errores_oob <- rbind(errores_oob,
                      data.frame(ntrees = ntree_actual,
                                error_oob = modelo$err.rate[ntree_actual, "OOB"]))
  
}

# Evaluamos cada modelo
for (i in seq_along(ntrees_valores)) {
  ntree_actual <- ntrees_valores[i]
  modelo <- modelos_rf[[as.character(ntree_actual)]]
  
  # Predicciones en Test
  pred_prob <- predict(modelo, Test, type = "prob")[,2]
  pred_class <- as.numeric(as.character(predict(modelo, Test, type = "class")))
  
  # Calcular métricas
  tabla_confusion <- table(Observado = Test$tipo, Predicho = pred_class)
  
  accuracy <- sum(diag(tabla_confusion)) / sum(tabla_confusion)
  precision <- ifelse(sum(tabla_confusion[,2]) > 0, 
                     tabla_confusion[2,2] / sum(tabla_confusion[,2]), 0)
  recall <- ifelse(sum(tabla_confusion[2,]) > 0, 
                  tabla_confusion[2,2] / sum(tabla_confusion[2,]), 0)
  f1 <- ifelse((precision + recall) > 0, 
              2 * precision * recall / (precision + recall), 0)
  roc_obj <- roc(Test$tipo, pred_prob, quiet = TRUE)
  auc_value <- auc(roc_obj)
  # Almacenar métricas
  metricas_por_ntrees <- rbind(metricas_por_ntrees,
                              data.frame(ntrees = ntree_actual,
                                        accuracy = accuracy,
                                        precision = precision,
                                        recall = recall,
                                        f1_score = f1,
                                        auc_roc = auc_value))
  
  # Almacenar predicciones para análisis posterior
  predicciones_por_modelo[[as.character(ntree_actual)]] <- list(
    prob = pred_prob,
    class = pred_class
  )
}


# Graficamos el error OOB
grafico_oob <- ggplot(errores_oob, aes(x = ntrees, y = error_oob)) +
  geom_line(color = "red", size = 1.2) +
  geom_point(color = "darkred", size = 3) +
  geom_text(aes(label = round(error_oob, 3)), 
            vjust = -0.5, hjust = 0.5, size = 3) +
  theme_minimal() +
  labs(title = "Error Out-of-Bag vs Número de Árboles",
       x = "Número de Árboles",
       y = "Error OOB") +
  scale_x_continuous(breaks = ntrees_valores)

# Graficamos métricas
metricas_long <- metricas_por_ntrees %>%
  tidyr::pivot_longer(cols = c(accuracy, precision, recall, f1_score),
                      names_to = "metrica", values_to = "valor")

grafico_metricas <- ggplot(metricas_long, aes(x = ntrees, y = valor, color = metrica)) +
  geom_line(size = 1.2) +
  geom_point(size = 3) +
  theme_minimal() +
  labs(title = "Métricas en Conjunto Test vs Número de Árboles",
       x = "Número de Árboles",
       y = "Valor de la Métrica",
       color = "Métrica") +
  scale_x_continuous(breaks = ntrees_valores) +
  scale_y_continuous(limits = c(0, 1)) +
  theme(legend.position = "bottom")


# Mostrar gráficos

print(grafico_oob)
print(grafico_metricas)
tabla_resumen <- merge(tiempos_entrenamiento, errores_oob, by = "ntrees")
tabla_resumen <- merge(tabla_resumen, metricas_por_ntrees, by = "ntrees")

# Formatear tabla para mostrar
tabla_mostrar <- tabla_resumen %>%
  mutate(
  
    error_oob = round(error_oob, 4),
    accuracy = round(accuracy, 3),
    precision = round(precision, 3),
    recall = round(recall, 3),
    f1_score = round(f1_score, 3),
    auc_roc = round(auc_roc, 3)
  ) %>%
  dplyr::select(ntrees, error_oob, accuracy, precision, recall, f1_score, auc_roc)

print(tabla_mostrar)


# Recomendar número óptimo de árboles
mejor_f1_idx <- which.max(metricas_por_ntrees$f1_score)
mejor_ntrees_f1 <- ntrees_valores[mejor_f1_idx]
mejor_f1 <- max(metricas_por_ntrees$f1_score)

mejor_accuracy_idx <- which.max(metricas_por_ntrees$accuracy)
mejor_ntrees_accuracy <- ntrees_valores[mejor_accuracy_idx]
mejor_accuracy <- max(metricas_por_ntrees$accuracy)

mejor_aucroc_idx <- which.max(metricas_por_ntrees$auc_roc)
mejor_ntrees_aucroc <- ntrees_valores[mejor_aucroc_idx]
mejor_aucroc <- max(metricas_por_ntrees$auc_roc)


cat("Mejor F1-Score:", mejor_ntrees_f1, "árboles (F1 =", round(mejor_f1, 3), ")\n")
cat("Mejor Accuracy:", mejor_ntrees_accuracy, "árboles (Acc =", round(mejor_accuracy, 3), ")\n")
cat("Mejor AUC-ROC:", mejor_ntrees_aucroc, "árboles (F1 =", round(mejor_aucroc, 3), ")\n")
```



```{r}
# Definir grid de parámetros
nrounds_grid <- seq(50, 100, by = 10)  # 50, 60, 70, 80, 90, 100
eta_grid <- c(0.1, 0.2, 0.3)
max_depth_fijo <- 6

# Crear grid completo de combinaciones
param_grid <- expand.grid(
  nrounds = nrounds_grid,
  eta = eta_grid,
  max_depth = max_depth_fijo,
  stringsAsFactors = FALSE
)

Train_xgb_debug <- preparar_para_xgboost(Train)

vars_problematicas <- sapply(Train_xgb_debug, function(x) !is.numeric(x) && names(Train_xgb_debug) != "tipo")

preparar_para_xgboost_robusto <- function(data) {
  data_prep <- data
  
  # Convertir TODAS las variables (excepto tipo) a numéricas
  for (col in names(data_prep)) {
    if (col != "tipo") {  
      if (is.character(data_prep[[col]])) {
        # Para character: convertir a factor primero, luego a numérico
        data_prep[[col]] <- as.numeric(as.factor(data_prep[[col]]))
      } else if (is.factor(data_prep[[col]])) {
        # Para factor: convertir directamente a numérico
        data_prep[[col]] <- as.numeric(data_prep[[col]])
      } else if (is.logical(data_prep[[col]])) {
        # Para logical: convertir a 0/1
        data_prep[[col]] <- as.numeric(data_prep[[col]])
      } else if (!is.numeric(data_prep[[col]])) {
        # Para cualquier otro tipo: forzar a numérico
        data_prep[[col]] <- as.numeric(as.character(data_prep[[col]]))
      }
      
      # Reemplazar NAs con 0 si se generaron durante la conversión
      data_prep[[col]][is.na(data_prep[[col]])] <- 0
    }
  }
  
  return(data_prep)
}

Train_xgb <- preparar_para_xgboost_robusto(Train)
Test_xgb <- preparar_para_xgboost_robusto(Test)
Validation_xgb <- preparar_para_xgboost_robusto(Validation)
preTrain_xgb <- preparar_para_xgboost_robusto(preTrain)

tipos_train <- sapply(Train_xgb[, setdiff(names(Train_xgb), "tipo")], class)

vars_no_numericas <- names(tipos_train)[tipos_train != "numeric" & tipos_train != "integer"]
if(length(vars_no_numericas) > 0) {
  cat("Variables que AÚN no son numéricas:\n")
  print(vars_no_numericas)
  
  # Forzar manualmente estas variables
  for(var in vars_no_numericas) {
    cat("Forzando", var, "a numérico...\n")
    Train_xgb[[var]] <- as.numeric(as.character(Train_xgb[[var]]))
    Test_xgb[[var]] <- as.numeric(as.character(Test_xgb[[var]]))
    Validation_xgb[[var]] <- as.numeric(as.character(Validation_xgb[[var]]))
    
    # Reemplazar NAs con 0
    Train_xgb[[var]][is.na(Train_xgb[[var]])] <- 0
    Test_xgb[[var]][is.na(Test_xgb[[var]])] <- 0
    Validation_xgb[[var]][is.na(Validation_xgb[[var]])] <- 0
  }
}


variables_predictoras <- setdiff(names(Train_xgb), "tipo")


dtrain <- xgb.DMatrix(data = as.matrix(Train_xgb[, variables_predictoras]), 
                      label = Train_xgb$tipo)
dtest <- xgb.DMatrix(data = as.matrix(Test_xgb[, variables_predictoras]), 
                     label = Test_xgb$tipo)
dvalidation <- xgb.DMatrix(data = as.matrix(Validation_xgb[, variables_predictoras]), 
                          label = Validation_xgb$tipo)

dpreTrain <- xgb.DMatrix(data = as.matrix(preTrain_xgb[, variables_predictoras]), 
                          label = preTrain_xgb$tipo)

# Inicializar estructuras para almacenar resultados
modelos_xgb <- list()
resultados_grid <- data.frame()
predicciones_por_modelo <- list()

set.seed(123)  # Para reproducibilidad

# Entrenamos modelos

for (i in 1:nrow(param_grid)) {
  params <- param_grid[i, ]
  nrounds_actual <- params$nrounds
  eta_actual <- params$eta
  max_depth_actual <- params$max_depth
  
  combo_name <- paste0("nr", nrounds_actual, "_eta", eta_actual, "_md", max_depth_actual)

 
  
  # Parámetros del modelo
  params_xgb <- list(
    objective = "binary:logistic",
    eval_metric = "logloss",
    eta = eta_actual,
    max_depth = max_depth_actual,
    subsample = 0.8,
    colsample_bytree = 1.0,
    seed = 123
  )
  
  # Entrenar modelo
  modelo <- xgb.train(
    params = params_xgb,
    data = dtrain,
    nrounds = nrounds_actual,
    watchlist = list(train = dtrain, eval = dtest),
    verbose = 0,
    early_stopping_rounds = 10
  )
  
 
  
  # Almacenar modelo
  modelos_xgb[[combo_name]] <- modelo
  
  # Hacer predicciones en Test
  pred_prob <- predict(modelo, dtest)
  pred_class <- ifelse(pred_prob > 0.5, 1, 0)
  
  # Calcular métricas
  tabla_confusion <- table(Observado = Test$tipo, Predicho = pred_class)
  
  accuracy <- sum(diag(tabla_confusion)) / sum(tabla_confusion)
  precision <- ifelse(sum(tabla_confusion[,2]) > 0, 
                     tabla_confusion[2,2] / sum(tabla_confusion[,2]), 0)
  recall <- ifelse(sum(tabla_confusion[2,]) > 0, 
                  tabla_confusion[2,2] / sum(tabla_confusion[2,]), 0)
  f1 <- ifelse((precision + recall) > 0, 
              2 * precision * recall / (precision + recall), 0)
  
  roc_obj <- roc(Test$tipo, pred_prob, quiet = TRUE)
  auc_value <- auc(roc_obj)
  # Obtener error de entrenamiento final
  train_error <- modelo$evaluation_log$train_logloss[nrow(modelo$evaluation_log)]
  eval_error <- modelo$evaluation_log$eval_logloss[nrow(modelo$evaluation_log)]
  
  # Almacenar resultados
  resultados_grid <- rbind(resultados_grid, data.frame(
    combo = combo_name,
    nrounds = nrounds_actual,
    eta = eta_actual,
    max_depth = max_depth_actual,
    tiempo_seg = tiempo_transcurrido,
    train_logloss = train_error,
    eval_logloss = eval_error,
    accuracy = accuracy,
    precision = precision,
    recall = recall,
    f1_score = f1,
    auc_roc = auc_value,
    stringsAsFactors = FALSE
  ))
  
  # Almacenar predicciones para análisis posterior
  predicciones_por_modelo[[combo_name]] <- list(
    prob = pred_prob,
    class = pred_class
  )

}

# Elegimos el mejor modelo 


# Encontrar mejores modelos
mejor_f1_idx <- which.max(resultados_grid$f1_score)
mejor_accuracy_idx <- which.max(resultados_grid$accuracy)
mejor_precision_idx <- which.max(resultados_grid$precision)
mejor_recall_idx <- which.max(resultados_grid$recall)
mejor_aucroc_idx <- which.max(resultados_grid$auc_roc)

cat("Mejor F1-Score:", round(resultados_grid$f1_score[mejor_f1_idx], 3), 
    "| Combo:", resultados_grid$combo[mejor_f1_idx], "\n")
cat("Mejor Accuracy:", round(resultados_grid$accuracy[mejor_accuracy_idx], 3), 
    "| Combo:", resultados_grid$combo[mejor_accuracy_idx], "\n")
cat("Mejor Precision:", round(resultados_grid$precision[mejor_precision_idx], 3), 
    "| Combo:", resultados_grid$combo[mejor_precision_idx], "\n")
cat("Mejor Recall:", round(resultados_grid$recall[mejor_recall_idx], 3), 
    "| Combo:", resultados_grid$combo[mejor_recall_idx], "\n")
cat("Mejor AUC-ROC:", round(resultados_grid$auc_roc[mejor_aucroc_idx], 3), 
    "| Combo:", resultados_grid$combo[mejor_aucroc_idx], "\n\n")


# Seleccionar top 3 modelos
mejor_modelo <- resultados_grid[order(resultados_grid$auc_roc, decreasing = TRUE)[1], ]
combo_name <- mejor_modelo$combo
  
# Obtener predicciones
pred_prob <- predicciones_por_modelo[[combo_name]]$prob
pred_class <- predicciones_por_modelo[[combo_name]]$class
  
# Crear shapefile para Test
datos_sf <- Test %>%
mutate(
  pred_prob_xgb = pred_prob,
  pred_class_xgb = pred_class,
  combo = combo_name,
  tipo_real = factor(tipo, levels = c(0, 1), labels = c("Hurto", "Robo")),
  tipo_pred_xgb = factor(pred_class_xgb, levels = c(0, 1), labels = c("Hurto", "Robo")),
  correcta_xgb = ifelse(tipo == pred_class_xgb, "Correcta", "Incorrecta")
) %>%
st_as_sf(coords = c("longitud", "latitud"), crs = 4326)

pred_data <- data.frame(
  longitud = Test$longitud,
  latitud = Test$latitud,
  prob_robo = predicciones_por_modelo[[combo_name]]$prob,
  tipo_real = factor(Test$tipo, levels = c(0, 1), labels = c("Hurto", "Robo"))
)



tabla_resumen_xgb <- resultados_grid %>%
  mutate(
    eval_logloss = round(eval_logloss, 4),
    accuracy = round(accuracy, 3),
    precision = round(precision, 3),
    recall = round(recall, 3),
    f1_score = round(f1_score, 3),
    auc_roc = round(auc_roc, 4)
  ) %>%
  arrange(desc(auc_roc))

print(tabla_resumen_xgb)

mejor_modelo <- resultados_grid[mejor_aucroc_idx, ]

cat("Combinación:", mejor_modelo$combo, "\n")
cat("nrounds =", mejor_modelo$nrounds, "| eta =", mejor_modelo$eta, 
    "| max_depth =", mejor_modelo$max_depth, "\n")
cat("F1-Score:", round(mejor_modelo$f1_score, 3), "\n")
cat("AUC-ROC:", round(mejor_modelo$auc_roc, 3), "\n")
```


Ahora que elegimos los mejores modelos comparemos resultados.


```{r}
# Entrenar el modelo de regresión logística
modelo_logistico <- glm(tipo ~ ., 
                       data = Train, 
                       family = binomial())

# Ver resumen del modelo
summary(modelo_logistico)

# Predecimos las respuestas
pred_train_prob <- predict(modelo_logistico, Train, type = "response")
pred_test_prob <- predict(modelo_logistico, Test, type = "response")

# Binarizamos
pred_train_class <- ifelse(pred_train_prob > 0.5, 1, 0)
pred_test_class <- ifelse(pred_test_prob > 0.5, 1, 0)

# Evaluar en todos los conjuntos
metricas_train <- calcular_metricas(Train$tipo, pred_train_class, pred_train_prob, "TRAIN")
metricas_test <- calcular_metricas(Test$tipo, pred_test_class,pred_test_prob ,"TEST")

shapefile_train <- crear_shapefile(Train, pred_train_prob, pred_train_class, "Train")
shapefile_test <- crear_shapefile(Test, pred_test_prob, pred_test_class, "Test")

mapas_train <- crear_mapa_comparativo(shapefile_train, "TRAIN")
mapas_test <- crear_mapa_comparativo(shapefile_test, "TEST")


grid.arrange(mapas_train$real, mapas_train$pred, ncol = 2)
grid.arrange(mapas_test$real, mapas_test$pred, ncol = 2)
grid.arrange(mapas_train$prob, mapas_train$accuracy, ncol = 2)
grid.arrange(mapas_test$prob, mapas_test$accuracy, ncol = 2)


# Estadísticas del Train
cat("=== ESTADÍSTICAS SHAPEFILE TRAIN ===\n")
cat("Total puntos:", nrow(shapefile_train), "\n")
cat("Predicciones correctas:", sum(shapefile_train$correcta == "Correcta"), "\n")
cat("Accuracy:", round(mean(shapefile_train$correcta == "Correcta"), 3), "\n")
cat("Distribución real - Hurtos:", sum(shapefile_train$tipo == 0), "| Robos:", sum(shapefile_train$tipo == 1), "\n")
cat("Distribución pred - Hurtos:", sum(shapefile_train$pred_class == 0), "| Robos:", sum(shapefile_train$pred_class == 1), "\n\n")

# Estadísticas del Test
cat("=== ESTADÍSTICAS SHAPEFILE TEST ===\n")
cat("Total puntos:", nrow(shapefile_test), "\n")
cat("Predicciones correctas:", sum(shapefile_test$correcta == "Correcta"), "\n")
cat("Accuracy:", round(mean(shapefile_test$correcta == "Correcta"), 3), "\n")
cat("Distribución real - Hurtos:", sum(shapefile_test$tipo == 0), "| Robos:", sum(shapefile_test$tipo == 1), "\n")
cat("Distribución pred - Hurtos:", sum(shapefile_test$pred_class == 0), "| Robos:", sum(shapefile_test$pred_class == 1), "\n\n")

```

```{r}
modelo <- randomForest(factor(tipo) ~ ., 
                        data = Train,
                        ntree = 300,
                        mtry = 2,
                        importance = TRUE,
                        do.trace = FALSE)

pred_train_prob<- predict(modelo, Train, type = "prob")[,2]
pred_train_class <- as.numeric(as.character(predict(modelo, Train, type = "class")))

pred_test_prob <- predict(modelo, Test, type = "prob")[,2]
pred_test_class <- as.numeric(as.character(predict(modelo, Test, type = "class")))

shapefile_train <- crear_shapefile(Train, pred_train_prob, pred_train_class, "Train")
shapefile_test <- crear_shapefile(Test, pred_test_prob, pred_test_class, "Test")


mapas_train <- crear_mapa_comparativo(shapefile_train, "TRAIN")
mapas_test <- crear_mapa_comparativo(shapefile_test, "TEST")

plot(roc(Train$tipo, pred_train_prob, quiet = TRUE), 
     main = paste("Curva ROC en Train") , 
     col = "blue", 
     lwd = 2,
     print.auc = TRUE)

plot(roc(Test$tipo, pred_test_prob, quiet = TRUE), 
   main = paste("Curva ROC en Test") , 
   col = "blue", 
   lwd = 2,
   print.auc = TRUE)

grid.arrange(mapas_train$real, mapas_train$pred, ncol = 2)
grid.arrange(mapas_test$real, mapas_test$pred, ncol = 2)
grid.arrange(mapas_train$prob, mapas_train$accuracy, ncol = 2)
grid.arrange(mapas_test$prob, mapas_test$accuracy, ncol = 2)
```


```{r}
params_xgb <- list(
    objective = "binary:logistic",
    eval_metric = "logloss",
    eta = 0.3,
    max_depth = 6,
    subsample = 0.8,
    colsample_bytree = 1.0,
    seed = 123
  )

modelo <- xgb.train(
  params = params_xgb,
  data = dtrain,
  nrounds = 60,
  watchlist = list(train = dtrain, eval = dtest),
  verbose = 0,
  early_stopping_rounds = 10
)


pred_train_prob <- predict(modelo, dtrain)
pred_train_class <- ifelse(pred_train_prob > 0.5, 1, 0)

pred_test_prob <- predict(modelo, dtest)
pred_test_class <- ifelse(pred_test_prob > 0.5, 1, 0)



shapefile_train <- crear_shapefile(Train, pred_train_prob, pred_train_class, "Train")
shapefile_test <- crear_shapefile(Test, pred_test_prob, pred_test_class, "Test")


mapas_train <- crear_mapa_comparativo(shapefile_train, "TRAIN")
mapas_test <- crear_mapa_comparativo(shapefile_test, "TEST")

plot(roc(Train$tipo, pred_train_prob, quiet = TRUE), 
     main = paste("Curva ROC en Train") , 
     col = "blue", 
     lwd = 2,
     print.auc = TRUE)

plot(roc(Test$tipo, pred_test_prob, quiet = TRUE), 
   main = paste("Curva ROC en Test") , 
   col = "blue", 
   lwd = 2,
   print.auc = TRUE)

grid.arrange(mapas_train$real, mapas_train$pred, ncol = 2)
grid.arrange(mapas_test$real, mapas_test$pred, ncol = 2)
grid.arrange(mapas_train$prob, mapas_train$accuracy, ncol = 2)
grid.arrange(mapas_test$prob, mapas_test$accuracy, ncol = 2)
```


Ahora que tenemos definidas las mejores configuraciones, determinemos con validación cuál es el mejor de estos tres modelos. Para determinar esto utilizaremos el AUC-ROC.

```{r}
modelo_logistico <- glm(tipo ~ ., 
                       data = preTrain, 
                       family = binomial())

# Predecimos las respuestas
pred_Validation_prob <- predict(modelo_logistico, Validation, type = "response")

# Binarizamos
pred_Validation_class <- ifelse(pred_Validation_prob > 0.5, 1, 0)


# Evaluar en todos los conjuntos
metricas_Validation <- calcular_metricas(Validation$tipo, pred_Validation_class,pred_Validation_prob ,"Validation")

shapefile_Validation <- crear_shapefile(Validation, pred_Validation_prob, pred_Validation_class, "Validation")

mapas_Validation <- crear_mapa_comparativo(shapefile_Validation, "VALIDATION")

grid.arrange(mapas_Validation$real, mapas_Validation$pred, ncol = 2)
grid.arrange(mapas_Validation$prob, mapas_Validation$accuracy, ncol = 2)

# Estadísticas del Test
cat("Total puntos:", nrow(shapefile_Validation), "\n")
cat("Predicciones correctas:", sum(shapefile_Validation$correcta == "Correcta"), "\n")
cat("Accuracy:", round(mean(shapefile_Validation$correcta == "Correcta"), 3), "\n")
cat("Distribución real - Hurtos:", sum(shapefile_Validation$tipo == 0), "| Robos:", sum(shapefile_Validation$tipo == 1), "\n")
cat("Distribución pred - Hurtos:", sum(shapefile_Validation$pred_class == 0), "| Robos:", sum(shapefile_Validation$pred_class == 1), "\n\n")
```


```{r}
modelo <- randomForest(factor(tipo) ~ ., 
                        data = preTrain,
                        ntree = 300,
                        mtry = 2,
                        importance = TRUE,
                        do.trace = FALSE)

pred_Validation_prob <- predict(modelo, Validation, type = "prob")[,2]
pred_Validation_class <- as.numeric(as.character(predict(modelo, Validation, type = "class")))
shapefile_Validation <- crear_shapefile(Validation, pred_Validation_prob, pred_Validation_class, "VALIDATION")
mapas_Validation <- crear_mapa_comparativo(shapefile_Validation, "VALIDATION")

plot(roc(Validation$tipo, pred_Validation_prob, quiet = TRUE), 
   main = paste("Curva ROC en VALIDATION") , 
   col = "blue", 
   lwd = 2,
   print.auc = TRUE)

grid.arrange(mapas_Validation$real, mapas_Validation$pred, ncol = 2)
grid.arrange(mapas_Validation$prob, mapas_Validation$accuracy, ncol = 2)
```

```{r}

params_xgb <- list(
    objective = "binary:logistic",
    eval_metric = "logloss",
    eta = 0.3,
    max_depth = 6,
    subsample = 0.8,
    colsample_bytree = 1.0,
    seed = 123
  )

modelo <- xgb.train(
  params = params_xgb,
  data = dpreTrain,
  nrounds = 60,
  watchlist = list(train = dpreTrain, eval = dvalidation),
  verbose = 0,
  early_stopping_rounds = 10
)

pred_validation_prob <- predict(modelo, dvalidation)
pred_validation_class <- ifelse(pred_validation_prob > 0.5, 1, 0)
shapefile_validation <- crear_shapefile(Validation, pred_validation_prob, pred_validation_class, "Validation")
mapas_test <- crear_mapa_comparativo(shapefile_validation, "VALIDATION")

plot(roc(Validation$tipo, pred_validation_prob, quiet = TRUE), 
   main = paste("Curva ROC en Validation") , 
   col = "blue", 
   lwd = 2,
   print.auc = TRUE)

grid.arrange(mapas_test$real, mapas_test$pred, ncol = 2)
grid.arrange(mapas_test$prob, mapas_test$accuracy, ncol = 2)
```
